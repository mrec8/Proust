"""
Interface to interact with large language models (LLM).
"""
import os
import time
import yaml
import json
from typing import Dict, List, Any, Optional, Union
from dotenv import load_dotenv
import openai

class LLMInterface:
    """
    Class to interact with large language models.
    """
    
    def __init__(self, config_path: str):
        """
        Initializes the LLM interface.
        
        Args:
            config_path: Path to the configuration file
        """
        # Load environment variables
        load_dotenv()
        
        # Get the API key
        self.api_key = os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key not found. Set the OPENAI_API_KEY environment variable.")
        
        # Configure client
        openai.api_key = self.api_key
        
        # Load configuration
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Specific configurations
        self.model = self.config['agents'].get('llm_model', 'gpt-3.5-turbo')
        self.default_temperature = self.config['agents'].get('temperature', 0.7)
        self.default_max_tokens = self.config['agents'].get('max_tokens', 1024)
        
        # Initialize call counter and cache
        self.call_count = 0
        self.cache = {}
        
        # Configure rate limit parameters
        self.rate_limit_per_minute = 20  # Adjust based on your plan
        self.last_call_timestamp = 0
    
    def generate(self, prompt: str, temperature: Optional[float] = None, 
                max_tokens: Optional[int] = None, use_cache: bool = True) -> str:
        """
        Generates text using the LLM.
        
        Args:
            prompt: Input text for the model
            temperature: Temperature for generation (higher value = more random)
            max_tokens: Maximum number of tokens to generate
            use_cache: Whether to use cache for identical prompts
            
        Returns:
            Text generated by the model
        """
        # Use default values if not specified
        temp = temperature if temperature is not None else self.default_temperature
        tokens = max_tokens if max_tokens is not None else self.default_max_tokens
        
        # Check if the result is in cache
        cache_key = f"{prompt}_{temp}_{tokens}"
        if use_cache and cache_key in self.cache:
            return self.cache[cache_key]
        
        # Manage rate limit
        self._manage_rate_limit()
        
        try:
            # Make the API call
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful and accurate assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=temp,
                max_tokens=tokens
            )
            
            # Extract the generated text
            generated_text = response.choices[0].message.content
            
            # Update call counter and timestamp
            self.call_count += 1
            self.last_call_timestamp = time.time()
            
            # Save to cache
            if use_cache:
                self.cache[cache_key] = generated_text
            
            return generated_text
        
        except Exception as e:
            print(f"Error generating text: {e}")
            # Return an error message or a default response
            return f"Error: Unable to generate text due to: {str(e)}"
    
    def _manage_rate_limit(self) -> None:
        """
        Manages the API call rate to avoid exceeding limits.
        """
        # Calculate time elapsed since the last call
        current_time = time.time()
        time_since_last_call = current_time - self.last_call_timestamp
        
        # If less than 60 seconds have passed and several calls have already been made
        if time_since_last_call < 60 and self.call_count >= self.rate_limit_per_minute:
            # Calculate necessary wait time
            wait_time = 60 - time_since_last_call
            print(f"Waiting {wait_time:.2f} seconds to respect API limits...")
            time.sleep(wait_time)
        
        # Reset counter if more than a minute has passed
        if time_since_last_call >= 60:
            self.call_count = 0